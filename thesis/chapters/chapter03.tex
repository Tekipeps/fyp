\comment{https://www.youtube.com/watch?v=sgQAhG5Q7iY}

The proposed system is divided into three major components shown in \figurename~\ref{fig:sytem-overview} which includes the web client, web server and machine learning models. The web interface get the user health data and sends it to the web server for processing. The web server passes the data to the trained model and returns the binary output which the user sees.

\begin{figure}[htb]
	\centering
	\makebox[0cm]{\includegraphics[scale=0.3]{system-overview.png}}
	\caption{System overview}
	\label{fig:sytem-overview}
\end{figure}


\section{Machine learning workflow}
\figurename~\ref{fig:ml-workflow} shows the workflow of for creating the machine learning model.

\begin{figure}[htb]
	\centering
	\makebox[0cm]{\includegraphics[scale=0.3]{ml-workflow.png}}
	\caption{Machine learning workflow}
	\label{fig:ml-workflow}
\end{figure}

\subsection{Dataset}
An estimated 17.9 million people die each year as a result of cardiovascular diseases (CVDs), making up 31\% of all deaths worldwide. Heart attacks and strokes are the cause of mortality for three-quarters of people under the age of 70 who die from CVD. Several CVDs can lead to heart failure, and this dataset comprises 11 variables that can be used to gauge the likelihood of heart failure \citep{world-health-organization-2019}.

An early diagnosis and management strategy can be of significant use to people with coronary heart disease or those at high risk of developing the illness because of the existence of risk factors such as hypertension, diabetes, and hyperlipidaemia.

\subsubsection{Features}
These features represent the different columns in the dataset
\begin{enumerate}[label=(\alph*)]
	\item{\textbf{Age:} age of the patient [years]}
	\item{\textbf{Sex:} sex of the patient [M: Male, F: Female]}
	\item{\textbf{ChestPainType:} chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]}
	\item{\textbf{RestingBP:} resting blood pressure [mm Hg]}
	\item{\textbf{Cholesterol:} serum cholesterol [mm/dl]}
	\item{\textbf{FastingBS:} fasting blood sugar [1: if FastingBS $>$ 120 mg/dl, 0: otherwise]}
	\item{\textbf{RestingECG:} resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]}
	\item{\textbf{MaxHR:} maximum heart rate achieved [Numeric value between 60 and 202]}
	\item{\textbf{ExerciseAngina:} exercise-induced angina [Y: Yes, N: No]}
	\item{\textbf{Oldpeak:} oldpeak = ST [Numeric value measured in depression]}
	\item{\textbf{ST\textunderscore Slope:} the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]}
	\item{\textbf{HeartDisease}:output class [1: heart disease, 0: Normal]}
\end{enumerate}

\subsubsection{Target variable}
In the context of machine learning, the term "target variable" refers to the variable that either is or should be the output. If you are classifying the data, for instance, it could be a binary 0 or 1, but if you are running a regression, it could be a continuous variable. In the field of statistics, this concept is also known as the response variable.

In the context of this research, the variable HeartDisease serves as our target variable. We are interested in identifying whether or not somebody is likely to get heart disease based on the input parameters such as gender, age, and the results of numerous tests.

\subsection{Exploratory data analysis}
Exploratory data analysis, (EDA) is the important process of looking at data for the first time to find patterns, find outliers, test hypotheses, and check assumptions with the help of summary statistics and graphical representations. This is done to make sure that the data are correct and can be reliable. 

\subsubsection{Why do we need EDA?}
The reasons for perfoming EDA are summarized below:

\begin{enumerate}
	\item Understanding the dataset helps clean up the dataset.
	\item It gives a clear idea of the features and how they fit together.
	\item Gives guidelines for what variables are important and what variables can be left out or taken away.
	\item Handling Missing values or human error.
	\item Identifying outliers.
	\item The EDA process would help to get the most information out of a dataset.
	\item EDA takes time but its very effective.
\end{enumerate}

\subsection{Data preprocessing}
Data preprocessing is required for accurate data representation and machine learning classifiers, for proper training and evaluation. For the dataset to be used effectively by the classifiers, preprocessing methods such imputation, feature scaling and handling of categorical variable have been applied.

\subsubsection{Imputation}
Imputation is a straightforward operation that involves replacing the values in our dataset that are absent (null). We can accomplish this by implementing our own specialized function, or we can simply perform imputation by making use of sklearn's built-in SimpleImputer class. Both of these options are available to use. For example:
\begin{lstlisting}[language=Python, numbers=none, label=Simple Imputer]
	from sklearn.impute import SimpleImputer
	
	imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
	imputer = imputer.fit(df[['Weight']])
	df['Weight'] = imputer.transform(df[['Weight']])
\end{lstlisting}

\subsubsection{Feature scaling}

Feature scaling is a method for uniformly reducing the values of each independent feature in our dataset. Algorithms can be calculated much more quickly with the aid of feature scaling making it a significant step in the preprocessing of data. The machine learning models would give larger weights to higher values and lower weights to lower values if we hadn't scaled the features. Additionally, training the machine learning model will take a longer time without scaling. The standard scalar is a feature scaling method that guarantees that they all have the same mean and variance. Although there were no features with missing values in the dataset, it is a recommended practice to check for and remove data units with missing values. These data preprocessing methods were all applied in this study.

\subsubsection{Handling of categorical variables}
Categorical variables/features are any feature type can be classified into two major types:
\begin{itemize}
	\item Nominal
	\item Ordinal
\end{itemize}
Nominal variables are those that have two or more categories that are not in any particular order. For example, gender can be thought of as a nominal variable if it is split into two groups, male and female. On the other hand, ordinal variables have "levels" or categories that are in a certain order. For example, an ordinal categorical variable can be a feature with three levels: low, medium, and high. Order is important.

This is a binary classification problem, and although the target is not skewed, we apply the best metric for it, which is the area under the ROC curve (AUC). Although precision and recall are also options, AUC combines precision and recall. As a result, we will use AUC to assess the model that we create using this dataset.

Since text data cannot be understood by computers, we must transform these categories to numbers. This may be done easily by using:

\begin{itemize}
	\item{Label Encoding: using sklearn's LabelEncoder class}
	\begin{lstlisting}[language=Python, caption=Label Encoder, numbers=none]
	from sklearn.preprocessing import LabelEncoder\end{lstlisting}
	\item{One Hot Encoding: using sklearn's OneHotEncoder class.}
	\begin{lstlisting}[language=Python, caption=Get dummies, numbers=none]
	from sklearn.preprocessing import OneHotEncoder\end{lstlisting}
\end{itemize}
But we must know when to apply which kind of label encoding: \\
\\
\noindent
\textbf{One-Hot Encoding is the best use for non-tree based Machine Learning Algorithms.}
\begin{itemize}
	\item The advantage of One-Hot-Encoding is that the result is binary rather than ordinal and that everything is in an orthogonal vector space.
	\item The negative is that with large cardinality, the feature space can quickly explode and you are forced to deal with the curse of dimensionality. In these circumstances, I usually use one-hot encoding followed by PCA to reduce dimensionality. Other encoding strategies rarely outperform the prudent combination of one-hot plus PCA, in my opinion. Because PCA detects linear overlap, it will naturally group comparable features together.
\end{itemize}

\noindent
\textbf{For tree based algorithms, label encoding is the best way to go.}

\begin{itemize}
	\item{LabelEncoder can convert [dog,cat,dog,mouse,cat] to [1,2,1,3,2], however the required ordinality means that cat is the average of dog and mouse. There are still algorithms that operate well with categorical variables, such as decision trees and random forests, and LabelEncoder can be used to store values with minimal disk space.}
\end{itemize}

\comment{

\subsubsection{Cross validation \& Hyperparameter optimisation}
In machine learning, choosing the best hyperparameters for a learning algorithm is called hyperparameter optimization or tuning. It is possible to regulate the learning process by altering a parameter's value, which is known as a hyperparameter. It's a different case when it comes to learning other parameters (usually node weights).
1
The same machine learning model may need various constraints, weights, or rates of learning to generalize to diverse patterns of input data. These parameters, also known as hyperparameters, must be fine-tuned to ensure that the model is able to tackle the machine learning problem to its full potential. To minimize a predetermined loss function on provided independent data, a hyperparameter optimization model is constructed by finding a pair of hyperparameters that minimizes the loss function. The loss associated with a tuple of hyperparameters is returned by the objective function. As a generalization test, cross-validation is commonly utilized.
}



\subsubsection{Cross validation}
In cross-validation, training data is split into a few parts. Some of these parts are used to train the model, and the rest are used to test it.

The best cross-validation method to use will depend on the dataset you are working with, and it may or may not be applicable to other datasets. There are a few cross-validation techniques, though, that are the most well-known and frequently employed. These consist of:

\begin{enumerate}[label=(\alph*)]
	\item K-fold cross-validation.
	\item Stratified k-fold cross-validation
\end{enumerate}

You shouldn't use random k-fold cross-validation if you have a skewed dataset for binary classification with 90\% positive samples and only 10\% negative samples. If you use simple k-fold cross-validation on a set of data like this, you might end up with folds that have no positive samples at all. We like to use stratified k-fold cross-validation in these situations. The number of labels in each fold stays the same with stratified k-fold cross-validation. So, you will have the same 90\% positive and 10\% negative samples in each fold. So, no matter what metric you use to measure, the results will be the same for all folds. So in the case, we would be using stratified k-fold cross-validation technique.

\comment{
\subsection{Training the models}
The following machine learning algorithms will be used for building the classification model.


}

\subsection{Performance evaluation}
When it comes to classification problems, the following measures are most frequently used:

\begin{enumerate}[label=(\alph*)]
	\item{Accuracy}
	\item{Precision (P)}
	\item{Recall (R)}
	\item{F1 score (F1)}
	\item{Area under the ROC (Receiver Operating Characteristic) curve or simply AUC (AUC)
		\begin{itemize}
			\item {When you have a dataset with skewed binary targets, another measure that is frequently employed is the area under the ROC curve calculation. The Area Under ROC Curve, Area Under Curve, or plain AUC are all names for this measure. The area under the ROC curve can be calculated in a variety of methods.}
			\item{AUC is a metric that is frequently used in machine learning for skewed binary classification tasks.}
		\end{itemize}
	}
\end{enumerate}

\section{Web interface}
The web interface consists of the web server and client components that provides a way for users to interact with the machine learning algorithms.

\begin{figure}[htb]
\centering
\makebox[0cm]{\includegraphics[scale=0.8]{web-interface.png}}
\caption{Web interface}
\label{fig:web-interface}
\end{figure}

A web server is software and hardware that responds to client requests sent over the World Wide Web using the HTTP (Hypertext Transfer Protocol) and other protocols. A web server's primary responsibility is to show website content by storing, processing, and sending webpages to users.

The web server is built in python with the python \href{https://flask.palletsprojects.com/en/2.1.x/}{flask} web framework using \href{https://jinja.palletsprojects.com/en/3.1.x/}{jinja} as a templating engine for the client interface. The server extracts the input data from a form the user submits via an API endpoint and validates the data so there is no invalid input. The implementations of the server will be discussed in \autoref{chap:4}.

\begin{figure}[htb]
	\centering
	\makebox[0cm]{\includegraphics[scale=0.3]{server.png}}
	\caption{Server}
	\label{fig:server}
\end{figure}

\subsection{Flask}
Flask is a lightweight Python web framework. It is called a micro-framework because you don't need any special tools or libraries to use it. It doesn't have a database abstraction layer, a form validation layer, or any other part where existing components are already provided by third-party libraries.
\subsection{Jinja}
Jinja is a Python programming language web template engine. Armin Ronacher developed it, and it is released under the BSD License. Jinja is comparable to the Django template engine, but it supports Python-like expressions and ensures that templates are evaluated in a sandbox.

\section{Inference engine}
In order to provide a response, an inference engine analyzes the facts included in the knowledge base and makes interpretations based on those evaluations. The inference engine in this study consists of a knowledge base and a parser. The knowledge base contains the expert knowledge encoded as rules in a yaml file. These encoded rules are then parsed by a parser which then generates inference based on the medical data.

\begin{figure}[htb]
	\centering
	\makebox[0cm]{\includegraphics[scale=0.3]{inference-engine.png}}
	\caption{Inference engine}
	\label{fig:inference-engine}
\end{figure}
